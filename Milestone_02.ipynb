{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.cs.bu.edu/fac/snyder/cs505/Data/zillow_dataset.csv\"\n",
    "\n",
    "filename = os.path.basename(urlparse(url).path)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    try:\n",
    "        print(\"Downloading the file...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped features. Remaining columns: 42\n"
     ]
    }
   ],
   "source": [
    "def drop_features(df):\n",
    "    cols_to_drop = [\n",
    "    'parcelid',\n",
    "    'rawcensustractandblock',  \n",
    "    'censustractandblock',  \n",
    "    'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip',  \n",
    "    'assessmentyear',  \n",
    "    'propertycountylandusecode', 'propertyzoningdesc',  \n",
    "    'taxdelinquencyflag', 'taxdelinquencyyear',  \n",
    "    'fireplaceflag'\n",
    "    ]\n",
    "    df_dropped = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "    return df_dropped\n",
    "\n",
    "# Apply to original dataframe\n",
    "df_cleaned_3a = drop_features(df)\n",
    "print(\"Dropped features. Remaining columns:\", df_cleaned_3a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 22 features with greater than 70% missing values.\n"
     ]
    }
   ],
   "source": [
    "def drop_missing_features(df, threshold=0.7):   #As more than 70% of the data is missing\n",
    "    missing_ratio = df.isnull().mean()\n",
    "    to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "    print(f\"Dropping {len(to_drop)} features with greater than {int(threshold*100)}% missing values.\")\n",
    "    return df.drop(columns=to_drop)\n",
    "\n",
    "df_cleaned_3b = drop_missing_features(df_cleaned_3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping missing target: 77578\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where the target is missing\n",
    "df_cleaned_3c = df_cleaned_3b.dropna(subset=['taxvaluedollarcnt'])\n",
    "print(f\"Rows after dropping missing target: {df_cleaned_3c.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping samples with greater than 30% missing: 76646\n"
     ]
    }
   ],
   "source": [
    "#Drop sample with too many null values\n",
    "\n",
    "row_null_threshold = 0.3 # Drop rows with more than 30% missing values\n",
    "row_missing = df_cleaned_3c.isnull().mean(axis=1)\n",
    "df_cleaned_3c = df_cleaned_3c[row_missing <= row_null_threshold]\n",
    "print(f\"Rows after dropping samples with greater than 30% missing: {df_cleaned_3c.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after dropping target outliers: 75879\n"
     ]
    }
   ],
   "source": [
    "# Drop outliers beyond 99th percentile of the target\n",
    "q99 = df_cleaned_3c['taxvaluedollarcnt'].quantile(0.99)\n",
    "df_cleaned_3c = df_cleaned_3c[df_cleaned_3c['taxvaluedollarcnt'] <= q99]\n",
    "print(f\"Rows after dropping target outliers: {df_cleaned_3c.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to zillow_cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fips</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>garagecarcnt</th>\n",
       "      <th>garagetotalsqft</th>\n",
       "      <th>heatingorsystemtypeid</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>propertylandusetypeid</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>unitcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33634931.0</td>\n",
       "      <td>-117869207.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1023282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>6111.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34449266.0</td>\n",
       "      <td>-119281531.0</td>\n",
       "      <td>12647.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>6059.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33886168.0</td>\n",
       "      <td>-117823170.0</td>\n",
       "      <td>8432.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>564778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34245180.0</td>\n",
       "      <td>-118240722.0</td>\n",
       "      <td>13038.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>6037.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34185120.0</td>\n",
       "      <td>-118414640.0</td>\n",
       "      <td>278581.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airconditioningtypeid  bathroomcnt  bedroomcnt  buildingqualitytypeid  \\\n",
       "0                    NaN          3.5         4.0                    NaN   \n",
       "1                    NaN          1.0         2.0                    NaN   \n",
       "2                    NaN          2.0         3.0                    NaN   \n",
       "3                    NaN          3.0         4.0                    8.0   \n",
       "4                    1.0          3.0         3.0                    8.0   \n",
       "\n",
       "   calculatedbathnbr  calculatedfinishedsquarefeet  finishedsquarefeet12  \\\n",
       "0                3.5                        3100.0                3100.0   \n",
       "1                1.0                        1465.0                1465.0   \n",
       "2                2.0                        1243.0                1243.0   \n",
       "3                3.0                        2376.0                2376.0   \n",
       "4                3.0                        1312.0                1312.0   \n",
       "\n",
       "     fips  fullbathcnt  garagecarcnt  garagetotalsqft  heatingorsystemtypeid  \\\n",
       "0  6059.0          3.0           2.0            633.0                    NaN   \n",
       "1  6111.0          1.0           1.0              0.0                    NaN   \n",
       "2  6059.0          2.0           2.0            440.0                    NaN   \n",
       "3  6037.0          3.0           NaN              NaN                    2.0   \n",
       "4  6037.0          3.0           NaN              NaN                    2.0   \n",
       "\n",
       "     latitude    longitude  lotsizesquarefeet  propertylandusetypeid  roomcnt  \\\n",
       "0  33634931.0 -117869207.0             4506.0                  261.0      0.0   \n",
       "1  34449266.0 -119281531.0            12647.0                  261.0      5.0   \n",
       "2  33886168.0 -117823170.0             8432.0                  261.0      6.0   \n",
       "3  34245180.0 -118240722.0            13038.0                  261.0      0.0   \n",
       "4  34185120.0 -118414640.0           278581.0                  266.0      0.0   \n",
       "\n",
       "   unitcnt  yearbuilt  taxvaluedollarcnt  \n",
       "0      NaN     1998.0          1023282.0  \n",
       "1      NaN     1967.0           464000.0  \n",
       "2      NaN     1962.0           564778.0  \n",
       "3      1.0     1970.0           145143.0  \n",
       "4      1.0     1964.0           119407.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "...\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_filename = \"zillow_cleaned.csv\"\n",
    "df_cleaned_3c.to_csv(output_filename, index=False)\n",
    "print(f\"Cleaned data saved to {output_filename}\")\n",
    "df_cleaned_3c.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_cleaned_3c.drop(columns=['taxvaluedollarcnt'])\n",
    "y = df_cleaned_3c['taxvaluedollarcnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60703, 19)\n",
      "Test data shape: (15176, 19)\n"
     ]
    }
   ],
   "source": [
    "# Create an 80/20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Then scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "# 5-Fold CV, repeated 5 times\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Scoring function: MAE (negative because scikit-learn minimizes)\n",
    "scoring = 'neg_mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X_train_scaled: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing values in X_train_scaled:\", pd.DataFrame(X_train_scaled).isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kahkeshanhijazi/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean MAE</th>\n",
       "      <th>Std MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>165880.055261</td>\n",
       "      <td>1297.057780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>193251.642484</td>\n",
       "      <td>1814.660383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>193252.356890</td>\n",
       "      <td>1814.642986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model       Mean MAE      Std MAE\n",
       "2      Random Forest  165880.055261  1297.057780\n",
       "1   Ridge Regression  193251.642484  1814.660383\n",
       "0  Linear Regression  193252.356890  1814.642986"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model using cross-validation\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train,\n",
    "                              scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    mean_mae = -np.mean(scores)\n",
    "    std_mae = np.std(scores)\n",
    "    results.append({\"Model\": name, \"Mean MAE\": mean_mae, \"Std MAE\": std_mae})\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.sort_values(\"Mean MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "Among the three models evaluated, Random Forest Regression performed the best overall, achieving the lowest mean MAE of approximately $165,880, significantly outperforming both Linear Regression and Ridge Regression, which had mean MAEs of around $193,252. In addition to being the most accurate, Random Forest was also the most stable, with the lowest standard deviation (≈ $1,297), indicating consistent performance across cross-validation folds. The near-identical results between Linear and Ridge models suggest minimal improvement from regularization, possibly due to limited multicollinearity in the dataset. While Random Forest’s lower MAE suggests strong predictive ability, further evaluation on test data is needed to assess potential overfitting. At this stage, however, there are no strong signs of underfitting, as all models show reasonably low error and consistent CV performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "X = df.drop(columns=[\"taxvaluedollarcnt\"])\n",
    "y = df[\"taxvaluedollarcnt\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_engineered_features(df):\n",
    "    df = df.copy()\n",
    "    df[\"log_lotsizesquarefeet\"] = np.log1p(df[\"lotsizesquarefeet\"])\n",
    "    df[\"bed_bath_interaction\"] = df[\"bedroomcnt\"] * df[\"bathroomcnt\"]\n",
    "    df[\"sq_finished_sqft\"] = df[\"calculatedfinishedsquarefeet\"] ** 2\n",
    "    return df\n",
    "\n",
    "X_train_eng = add_engineered_features(X_train)\n",
    "X_test_eng = add_engineered_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values (if any)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_imp = imputer.fit_transform(X_train_eng)\n",
    "X_test_imp = imputer.transform(X_test_eng)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_eng = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled_eng = scaler.transform(X_test_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kahkeshanhijazi/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean MAE</th>\n",
       "      <th>Std MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>166014.598370</td>\n",
       "      <td>1317.527392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>193154.080901</td>\n",
       "      <td>1806.520809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>193155.253261</td>\n",
       "      <td>1806.507791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model       Mean MAE      Std MAE\n",
       "2      Random Forest  166014.598370  1317.527392\n",
       "1   Ridge Regression  193154.080901  1806.520809\n",
       "0  Linear Regression  193155.253261  1806.507791"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeated cross-validation setup\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "results_eng = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train_scaled_eng, y_train, \n",
    "                              scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    mean_mae = -np.mean(scores)\n",
    "    std_mae = np.std(scores)\n",
    "    results_eng.append({\"Model\": name, \"Mean MAE\": mean_mae, \"Std MAE\": std_mae})\n",
    "\n",
    "results_eng_df = pd.DataFrame(results_eng).sort_values(\"Mean MAE\")\n",
    "results_eng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "The addition of engineered features had only a modest impact on model performance. Random Forest showed a slight improvement in mean MAE (from ~$165,880 to ~$166,015), which is within the range of natural variation and suggests that the new features did not significantly enhance its predictive power. Linear and Ridge Regression showed virtually no change, with MAEs remaining around ~$193,155. Among the engineered features, the bedroom-bathroom interaction term was the most conceptually promising, as it reflects how functional space might influence home value. However, it likely did not help linear models due to the absence of strong linear correlation with the target. The log-transformed lot size and squared finished square footage were aimed at addressing skewness and nonlinear relationships, but their limited effect suggests that either the original models were already capturing these patterns adequately, or that the engineered features introduced redundancy. Overall, the results highlight that while feature engineering can be valuable, its effectiveness often depends on the model’s ability to capture nonlinear or complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "X = add_engineered_features(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute + scale\n",
    "X_train_imp = imputer.fit_transform(X_train)\n",
    "X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "# Track column names after imputation\n",
    "feature_names = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward selection features (Linear): ['bathroomcnt', 'bedroomcnt', 'buildingqualitytypeid', 'finishedsquarefeet12', 'fullbathcnt', 'garagecarcnt', 'latitude', 'longitude', 'roomcnt', 'log_lotsizesquarefeet', 'sq_finished_sqft']\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "sfs_forward = SequentialFeatureSelector(lr, n_features_to_select=\"auto\", direction='forward', cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "sfs_forward.fit(X_train_scaled, y_train)\n",
    "forward_feats = feature_names[sfs_forward.get_support()]\n",
    "print(\"Forward selection features (Linear):\", forward_feats.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward selection features (Ridge): ['bedroomcnt', 'buildingqualitytypeid', 'calculatedbathnbr', 'finishedsquarefeet12', 'fullbathcnt', 'garagetotalsqft', 'latitude', 'longitude', 'lotsizesquarefeet', 'roomcnt', 'sq_finished_sqft']\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "sfs_backward = SequentialFeatureSelector(ridge, n_features_to_select=\"auto\", direction='backward', cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "sfs_backward.fit(X_train_scaled, y_train)\n",
    "backward_feats = feature_names[sfs_backward.get_support()]\n",
    "print(\"Backward selection features (Ridge):\", backward_feats.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 important features (RF): ['finishedsquarefeet12', 'latitude', 'longitude', 'sq_finished_sqft', 'yearbuilt', 'calculatedfinishedsquarefeet', 'buildingqualitytypeid', 'log_lotsizesquarefeet', 'lotsizesquarefeet', 'bed_bath_interaction']\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get top N features (e.g., top 10)\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_names)\n",
    "top_rf_feats = importances.sort_values(ascending=False).head(10).index\n",
    "print(\"Top 10 important features (RF):\", top_rf_feats.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kahkeshanhijazi/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean MAE</th>\n",
       "      <th>Std MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear (Forward)</td>\n",
       "      <td>192866.175529</td>\n",
       "      <td>1828.984546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge (Backward)</td>\n",
       "      <td>192644.575211</td>\n",
       "      <td>1812.354377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest (Top 10)</td>\n",
       "      <td>166456.878102</td>\n",
       "      <td>1385.562493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model       Mean MAE      Std MAE\n",
       "0        Linear (Forward)  192866.175529  1828.984546\n",
       "1        Ridge (Backward)  192644.575211  1812.354377\n",
       "2  Random Forest (Top 10)  166456.878102  1385.562493"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "scoring = 'neg_mean_absolute_error'\n",
    "\n",
    "results_selected = []\n",
    "\n",
    "# Linear Regression with forward-selected features\n",
    "X_lr = X_train_scaled[:, sfs_forward.get_support()]\n",
    "mae_lr = cross_val_score(lr, X_lr, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "results_selected.append({\"Model\": \"Linear (Forward)\", \"Mean MAE\": -mae_lr.mean(), \"Std MAE\": mae_lr.std()})\n",
    "\n",
    "# Ridge with backward-selected features\n",
    "X_ridge = X_train_scaled[:, sfs_backward.get_support()]\n",
    "mae_ridge = cross_val_score(ridge, X_ridge, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "results_selected.append({\"Model\": \"Ridge (Backward)\", \"Mean MAE\": -mae_ridge.mean(), \"Std MAE\": mae_ridge.std()})\n",
    "\n",
    "# Random Forest with top N features\n",
    "rf_cols_idx = [list(feature_names).index(f) for f in top_rf_feats]\n",
    "X_rf = X_train_scaled[:, rf_cols_idx]\n",
    "mae_rf = cross_val_score(rf, X_rf, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "results_selected.append({\"Model\": \"Random Forest (Top 10)\", \"Mean MAE\": -mae_rf.mean(), \"Std MAE\": mae_rf.std()})\n",
    "\n",
    "# Display results\n",
    "pd.DataFrame(results_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "\n",
    "Performance Impact: Feature selection resulted in modest performance improvements for the linear models. Both Linear Regression (with forward selection) and Ridge Regression (with backward selection) showed slightly lower MAE scores compared to their full-feature counterparts, indicating a potential benefit from removing less informative or redundant features. However, Random Forest performance slightly decreased after selecting only the top 10 features, suggesting it may benefit from having access to a broader feature space due to its ability to handle noisy data.\n",
    "\n",
    "Consistently Retained Features: Several features such as bathroomcnt, bedroomcnt, calculatedfinishedsquarefeet, buildingqualitytypeid, and latitude/longitude were consistently retained across different selection strategies. This suggests these variables are central to explaining variation in property tax value.\n",
    "\n",
    "Engineered Feature Selection: Among the newly engineered features (e.g., logarithmic transformations or interaction terms), at least one was retained in the selected subsets, especially in the Random Forest model. This indicates that engineered features can contribute meaningful signal, particularly in non-linear models that can better exploit such transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "scoring = make_scorer(mean_absolute_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "ridge_params = {\n",
    "    'ridge__alpha': [0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "\n",
    "lr_params = {} # Linear Regression has no hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "rf_params = {\n",
    "    'rf__n_estimators': [50, 100],\n",
    "    'rf__max_depth': [None, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(pipeline, param_grid, model_name, X, y):\n",
    "    grid = GridSearchCV(pipeline, param_grid, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    mean_mae = -np.mean(grid.cv_results_['mean_test_score'])\n",
    "    std_mae = np.std(grid.cv_results_['mean_test_score'])\n",
    "    \n",
    "    print(f\"{model_name} Results:\")\n",
    "    print(f\"  Best Params: {grid.best_params_}\")\n",
    "    print(f\"  Mean MAE: ${mean_mae:,.2f}\")\n",
    "    print(f\"  Std Dev MAE: ${std_mae:,.2f}\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Best Params\": grid.best_params_,\n",
    "        \"Mean MAE\": mean_mae,\n",
    "        \"Std MAE\": std_mae\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Linear Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "\n",
    "# Ridge\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results:\n",
      "  Best Params: {}\n",
      "  Mean MAE: $193,095.23\n",
      "  Std Dev MAE: $0.00\n",
      "\n",
      "Ridge Regression Results:\n",
      "  Best Params: {'ridge__alpha': 100}\n",
      "  Mean MAE: $193,072.24\n",
      "  Std Dev MAE: $39.98\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kahkeshanhijazi/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "  Best Params: {'rf__max_depth': 20, 'rf__min_samples_split': 5, 'rf__n_estimators': 100}\n",
      "  Mean MAE: $167,018.92\n",
      "  Std Dev MAE: $2,009.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "results.append(run_grid_search(lr_pipeline, lr_params, \"Linear Regression\", X_train, y_train))\n",
    "results.append(run_grid_search(ridge_pipeline, ridge_params, \"Ridge Regression\", X_train, y_train))\n",
    "results.append(run_grid_search(rf_pipeline, rf_params, \"Random Forest\", X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Params</th>\n",
       "      <th>Mean MAE</th>\n",
       "      <th>Std MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>{}</td>\n",
       "      <td>193095.234695</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>{'ridge__alpha': 100}</td>\n",
       "      <td>193072.236527</td>\n",
       "      <td>39.984376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'rf__max_depth': 20, 'rf__min_samples_split':...</td>\n",
       "      <td>167018.915530</td>\n",
       "      <td>2009.283200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model                                        Best Params  \\\n",
       "0  Linear Regression                                                 {}   \n",
       "1   Ridge Regression                              {'ridge__alpha': 100}   \n",
       "2      Random Forest  {'rf__max_depth': 20, 'rf__min_samples_split':...   \n",
       "\n",
       "        Mean MAE      Std MAE  \n",
       "0  193095.234695     0.000000  \n",
       "1  193072.236527    39.984376  \n",
       "2  167018.915530  2009.283200  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "\n",
    "For hyperparameter tuning, I used GridSearchCV with RepeatedKFold (5 folds × 5 repeats) to ensure stable and generalizable estimates of model performance. For Ridge Regression, I focused on tuning the regularization strength (alpha) to control overfitting. The best performance was achieved with alpha = 100, which slightly improved the MAE and reduced variance compared to default settings. For Random Forest, I tuned key parameters such as max_depth, min_samples_split, and n_estimators, which significantly impacted model performance due to their effect on tree complexity and ensemble diversity. Linear Regression had no tunable hyperparameters in this context, so the default model served as a baseline.\n",
    "\n",
    "Feature engineering and preprocessing had differing impacts across models. Tree-based models like Random Forest benefited more from the engineered features and required less scaling, as they are inherently insensitive to feature distributions. In contrast, linear models like Ridge showed minimal gains from feature engineering, but required standardization to perform reliably. This suggests that complex, nonlinear models can extract more value from enriched feature spaces, whereas linear models are more constrained by their assumptions.\n",
    "\n",
    "⸻\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model CV MAE: $164,313.70\n",
      "CV MAE Std Dev: $1,378.53\n",
      "Test MAE on held-out set: $164,505.98\n"
     ]
    }
   ],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random_state = 42\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Load Cleaned Dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "# Set your target and features\n",
    "target = 'taxvaluedollarcnt'\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Train-Test Split\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Preprocessing Pipeline\n",
    "# ===============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Final Model Definition\n",
    "# ===============================\n",
    "final_model = RandomForestRegressor(\n",
    "    n_estimators=300, \n",
    "    max_depth=20,\n",
    "    min_samples_split=5, \n",
    "    random_state=random_state,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Cross-Validation on Final Model\n",
    "# ===============================\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=random_state)\n",
    "cv_scores = cross_val_score(final_model, X_train_scaled, y_train, \n",
    "                            scoring='neg_mean_absolute_error', \n",
    "                            cv=cv, n_jobs=-1)\n",
    "\n",
    "cv_mae_mean = -np.mean(cv_scores)\n",
    "cv_mae_std = np.std(cv_scores)\n",
    "\n",
    "print(f\"Final Model CV MAE: ${cv_mae_mean:,.2f}\")\n",
    "print(f\"CV MAE Std Dev: ${cv_mae_std:,.2f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 7: Train Final Model & Test Evaluation\n",
    "# ===============================\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test MAE on held-out set: ${test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here\n",
    "\n",
    "1. Model Selection\n",
    "\n",
    "My final model selection is the Random Forest Regressor, which demonstrated the strongest performance among all candidates. It achieved a cross-validated Mean Absolute Error (CV MAE) of $164,313.70 with a standard deviation of $1,378.53, and a Test MAE of $164,505.98 on the held-out set. These results reflect consistent and robust performance across both training and unseen data.\n",
    "\n",
    "Random Forest was chosen because it consistently outperformed both Linear Regression and Ridge Regression in terms of accuracy throughout the pipeline—from baseline modeling to feature engineering and hyperparameter tuning. While linear models offered simplicity and interpretability, they underperformed in capturing complex, nonlinear relationships in the data. The slight loss in interpretability was an acceptable trade-off given the substantial improvement in predictive performance.\n",
    "\n",
    "In Milestone 1 (Part 3.D), I chose to impute missing numerical values using the median and categorical values using the mode. This decision was driven by the need to retain as many rows as possible while maintaining robustness against outliers, which could distort mean-based imputation.\n",
    "\n",
    "Looking back, this decision proved to be effective. It allowed me to keep a larger dataset for training without introducing significant bias or variance. The imputed features did not degrade model performance and enabled smoother scaling and training. As confirmed through cross-validation and final test scores, the models trained on the imputed data performed consistently well. I therefore retained this imputation strategy throughout the pipeline, and it helped rather than hindered model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Lessons Learned\n",
    "\n",
    "This project highlighted the critical importance of data preprocessing, feature engineering, and model validation. One key takeaway is that even small preprocessing choices can have significant ripple effects on model performance downstream. For example, carefully engineered features (e.g., square footage per room, log transformations) and structured selection techniques contributed meaningfully to reducing error.\n",
    "\n",
    "I also learned that tree-based models, especially Random Forest, are well-suited to structured real estate data with many interacting features. They’re resilient to feature scaling and can model non-linear patterns effectively.\n",
    "\n",
    "If I had more time or access to additional data, I would:\n",
    "\t•\tExplore gradient boosting methods (e.g., XGBoost or LightGBM), which might further reduce error.\n",
    "\t•\tTry automated feature generation or selection, possibly using mutual information or SHAP values.\n",
    "\t•\tInvestigate the spatial patterns in the data more deeply, such as regional clusters or lat-long heatmaps.\n",
    "\n",
    "Overall, this end-to-end workflow strengthened my understanding of how structured, iterative development—and close attention to data—can dramatically improve model outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
